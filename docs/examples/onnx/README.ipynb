{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Serving ONNX models\n",
        "\n",
        "Out of the box, `mlserver` supports the deployment and serving of `onnx` models.\n",
        "By default, it will assume that these models have been [serialised as ONNX format files](https://onnx.ai/onnx/intro/converters.html).\n",
        "\n",
        "In this example, we will cover how we can create and serialise a simple ONNX model, to then serve it using `mlserver`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "The first step will be to train a simple model using PyTorch and then convert it to ONNX format.\n",
        "For that, we will use a simple linear regression model trained on synthetic data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=4, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Define a simple linear model\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegressionModel(4, 1)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"Final training loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving our trained model\n",
        "\n",
        "To save our trained model in ONNX format, we will use PyTorch's ONNX export functionality.\n",
        "This is the [recommended approach by the ONNX project](https://onnx.ai/onnx/intro/converters.html).\n",
        "\n",
        "Our model will be persisted as a file named `linear-model.onnx`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.onnx\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create a dummy input for export (batch_size=1, features=4)\n",
        "dummy_input = torch.randn(1, 4)\n",
        "\n",
        "# Export the model to ONNX format\n",
        "model_file_name = \"linear-model.onnx\"\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    model_file_name,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        ")\n",
        "\n",
        "print(f\"Model saved as {model_file_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Serving\n",
        "\n",
        "Now that we have trained and saved our model, the next step will be to serve it using `mlserver`. \n",
        "For that, we will need to create 2 configuration files: \n",
        "\n",
        "- `settings.json`: holds the configuration of our server (e.g. ports, log level, etc.).\n",
        "- `model-settings.json`: holds the configuration of our model (e.g. input type, runtime to use, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### `settings.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile settings.json\n",
        "{\n",
        "    \"debug\": \"true\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### `model-settings.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile model-settings.json\n",
        "{\n",
        "    \"name\": \"linear-model\",\n",
        "    \"implementation\": \"mlserver_onnx.OnnxModel\",\n",
        "    \"parameters\": {\n",
        "        \"uri\": \"./linear-model.onnx\",\n",
        "        \"version\": \"v0.1.0\"\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Start serving our model\n",
        "\n",
        "Now that we have our config in-place, we can start the server by running `mlserver start .`. This needs to either be ran from the same directory where our config files are or pointing to the folder where they are.\n",
        "\n",
        "```shell\n",
        "mlserver start .\n",
        "```\n",
        "\n",
        "Since this command will start the server and block the terminal, waiting for requests, this will need to be ran in the background on a separate terminal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Send test inference request\n",
        "\n",
        "We now have our model being served by `mlserver`.\n",
        "To make sure that everything is working as expected, let's send a request from our test set.\n",
        "\n",
        "The request input `name` must match the ONNX model's input name (we used `\"input\"` in the export above). For that, we can use the Python types that `mlserver` provides out of box, or we can build our request manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "x_0 = X_test[0:1]\n",
        "inference_request = {\n",
        "    \"inputs\": [\n",
        "        {\"name\": \"input\", \"shape\": x_0.shape, \"datatype\": \"FP32\", \"data\": x_0.tolist()}\n",
        "    ]\n",
        "}\n",
        "\n",
        "endpoint = \"http://localhost:8080/v2/models/linear-model/versions/v0.1.0/infer\"\n",
        "response = requests.post(endpoint, json=inference_request, timeout=30)\n",
        "\n",
        "response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see above, the model predicted a value for the input, which we can compare with the actual test value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Predicted: {response.json()['outputs'][0]['data'][0]:.4f}\")\n",
        "print(f\"Actual: {y_test[0]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Requesting specific outputs\n",
        "\n",
        "ONNX models expose named outputs. By default, MLServer returns the first output\n",
        "using the `predict` alias. To request a specific output, define it explicitly.\n",
        "\n",
        "Example request:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "x_0 = X_test[0:1]\n",
        "inference_request = {\n",
        "    \"inputs\": [\n",
        "        {\"name\": \"input\", \"shape\": x_0.shape, \"datatype\": \"FP32\", \"data\": x_0.tolist()}\n",
        "    ],\n",
        "    \"outputs\": [{\"name\": \"output\"}],\n",
        "}\n",
        "\n",
        "endpoint = \"http://localhost:8080/v2/models/linear-model/versions/v0.1.0/infer\"\n",
        "response = requests.post(endpoint, json=inference_request, timeout=30)\n",
        "\n",
        "response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Execution providers and session options\n",
        "\n",
        "If you need to tune threading or runtime options, configure ONNX Runtime via\n",
        "`parameters.extra`.\n",
        "\n",
        "Example configuration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile model-settings.json\n",
        "{\n",
        "  \"name\": \"linear-model\",\n",
        "  \"implementation\": \"mlserver_onnx.OnnxModel\",\n",
        "  \"parameters\": {\n",
        "    \"uri\": \"./linear-model.onnx\",\n",
        "    \"version\": \"v0.1.0\",\n",
        "    \"extra\": {\n",
        "      \"providers\": [\"CPUExecutionProvider\"],\n",
        "      \"provider_options\": [{}],\n",
        "      \"session_options\": {\n",
        "        \"intra_op_num_threads\": 1,\n",
        "        \"inter_op_num_threads\": 1\n",
        "      },\n",
        "      \"session_config_entries\": {\n",
        "        \"session.set_denormal_as_zero\": \"1\"\n",
        "      },\n",
        "      \"run_options\": {\n",
        "        \"log_severity_level\": 3\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
